{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Text classification\n",
    "\n",
    "Text is a type of unstructure data. Machine Learning algorithms do not work well with any type of variable different from numbers, and text is all but numbers. Recently, in the NLP field, most of the new discoveries have been made thanks to defining new Deep Learning architectures to generate high quality word embeddings. They are a set of feature vector representations where words from a vocabulary are mapped to vector of real numbers. It all started in 2013 when Mikolov et. al. first introduced the Skip-gram model [1]. From Word2vec, several new model were developed such as ELMo [2] and BERT [3], the actual state-of-the-art model, among others.\n",
    "\n",
    "After a bit of background, the idea to solve this task is to use BERT or Universal Sentence Encoder (USE) [4] to encode the documents into vectors that can be feed into a Machine Learning classifier. Moreover, we need a model that understand, at least, our 3 languages. Then, we need a multilingual model.\n",
    "\n",
    "From the exploratory analysis, we recommend to use a pretrained model. There are plenty of them in Tensorflow Hub, in particular, Multilingual BERT and Multilingual USE [5] are available. Because MUSE does not required preprocessing, as the input data does not need to be masked, we are going to use it first. That is the reason why this task is divided in two jupyter notebooks, one in Google colab (**MUSE- Word Embeddings**)to create the word embeddings because MUSE uses a library (tensorflow_text) that does not work in windows, and the second one is this notebook run locally. \n",
    "\n",
    "### 1) Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Read the csv\n",
    "\n",
    "This csv is the output from the notebook run in Google Colab.\n",
    "\n",
    "The word embedding is flatten, generating as many columns as the length of the vector (512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f= pd.read_csv('ea_embeddings_muse_flatten.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "      <td>-0.019496</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>-0.011202</td>\n",
       "      <td>0.040384</td>\n",
       "      <td>-0.076767</td>\n",
       "      <td>0.066849</td>\n",
       "      <td>0.036627</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043784</td>\n",
       "      <td>-0.007727</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>-0.046863</td>\n",
       "      <td>-0.003643</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>-0.073423</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>0.023028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "      <td>-0.002822</td>\n",
       "      <td>-0.045204</td>\n",
       "      <td>-0.016848</td>\n",
       "      <td>0.053938</td>\n",
       "      <td>0.039466</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>-0.025186</td>\n",
       "      <td>-0.028859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048659</td>\n",
       "      <td>0.073567</td>\n",
       "      <td>-0.085638</td>\n",
       "      <td>-0.064839</td>\n",
       "      <td>-0.070530</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>-0.018194</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.045972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "      <td>0.026813</td>\n",
       "      <td>-0.011513</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>0.043837</td>\n",
       "      <td>0.051915</td>\n",
       "      <td>0.044183</td>\n",
       "      <td>-0.004949</td>\n",
       "      <td>-0.061447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032723</td>\n",
       "      <td>-0.029836</td>\n",
       "      <td>-0.032390</td>\n",
       "      <td>-0.014507</td>\n",
       "      <td>-0.048853</td>\n",
       "      <td>-0.016512</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.072263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "      <td>-0.054095</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.019378</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.024880</td>\n",
       "      <td>0.032522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045528</td>\n",
       "      <td>-0.062564</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>-0.019202</td>\n",
       "      <td>-0.047049</td>\n",
       "      <td>-0.063975</td>\n",
       "      <td>-0.059755</td>\n",
       "      <td>-0.069993</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>-0.015806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>APR</td>\n",
       "      <td>0.044433</td>\n",
       "      <td>0.037699</td>\n",
       "      <td>-0.054426</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>-0.046270</td>\n",
       "      <td>0.053568</td>\n",
       "      <td>0.022592</td>\n",
       "      <td>-0.059755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>-0.014587</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.032198</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.059288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  language category         2         3         4         5         6  \\\n",
       "0       en      APR -0.019496  0.028128 -0.011202  0.040384 -0.076767   \n",
       "1       en      APR -0.002822 -0.045204 -0.016848  0.053938  0.039466   \n",
       "2       en      APR  0.026813 -0.011513 -0.018855  0.043837  0.051915   \n",
       "3       en      APR -0.054095  0.026708 -0.001139  0.000139 -0.019378   \n",
       "4       en      APR  0.044433  0.037699 -0.054426 -0.018390 -0.046270   \n",
       "\n",
       "          7         8         9  ...       504       505       506       507  \\\n",
       "0  0.066849  0.036627 -0.000093  ... -0.043784 -0.007727  0.005836 -0.046863   \n",
       "1  0.065693 -0.025186 -0.028859  ...  0.048659  0.073567 -0.085638 -0.064839   \n",
       "2  0.044183 -0.004949 -0.061447  ...  0.032723 -0.029836 -0.032390 -0.014507   \n",
       "3  0.063965  0.024880  0.032522  ...  0.045528 -0.062564  0.006448 -0.019202   \n",
       "4  0.053568  0.022592 -0.059755  ...  0.045455  0.008011 -0.014587  0.044229   \n",
       "\n",
       "        508       509       510       511       512       513  \n",
       "0 -0.003643  0.009728 -0.000819 -0.073423  0.030665  0.023028  \n",
       "1 -0.070530 -0.083139  0.011784 -0.018194  0.042321  0.045972  \n",
       "2 -0.048853 -0.016512  0.011402 -0.006926  0.050542  0.072263  \n",
       "3 -0.047049 -0.063975 -0.059755 -0.069993  0.065400 -0.015806  \n",
       "4  0.035727  0.030701  0.032198 -0.065506  0.039980  0.059288  \n",
       "\n",
       "[5 rows x 514 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pd.concat to join the new columns with your original dataframe\n",
    "df_f = pd.concat([df_f,pd.get_dummies(df_f['language'], prefix='language')],axis=1)\n",
    "\n",
    "# now drop the original 'country' column (you don't need it anymore)\n",
    "df_f.drop(['language'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>language_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APR</td>\n",
       "      <td>-0.019496</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>-0.011202</td>\n",
       "      <td>0.040384</td>\n",
       "      <td>-0.076767</td>\n",
       "      <td>0.066849</td>\n",
       "      <td>0.036627</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.061016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046863</td>\n",
       "      <td>-0.003643</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>-0.073423</td>\n",
       "      <td>0.030665</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APR</td>\n",
       "      <td>-0.002822</td>\n",
       "      <td>-0.045204</td>\n",
       "      <td>-0.016848</td>\n",
       "      <td>0.053938</td>\n",
       "      <td>0.039466</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>-0.025186</td>\n",
       "      <td>-0.028859</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064839</td>\n",
       "      <td>-0.070530</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>-0.018194</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.045972</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APR</td>\n",
       "      <td>0.026813</td>\n",
       "      <td>-0.011513</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>0.043837</td>\n",
       "      <td>0.051915</td>\n",
       "      <td>0.044183</td>\n",
       "      <td>-0.004949</td>\n",
       "      <td>-0.061447</td>\n",
       "      <td>-0.014167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014507</td>\n",
       "      <td>-0.048853</td>\n",
       "      <td>-0.016512</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>0.050542</td>\n",
       "      <td>0.072263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APR</td>\n",
       "      <td>-0.054095</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.019378</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.024880</td>\n",
       "      <td>0.032522</td>\n",
       "      <td>-0.010525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019202</td>\n",
       "      <td>-0.047049</td>\n",
       "      <td>-0.063975</td>\n",
       "      <td>-0.059755</td>\n",
       "      <td>-0.069993</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>-0.015806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APR</td>\n",
       "      <td>0.044433</td>\n",
       "      <td>0.037699</td>\n",
       "      <td>-0.054426</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>-0.046270</td>\n",
       "      <td>0.053568</td>\n",
       "      <td>0.022592</td>\n",
       "      <td>-0.059755</td>\n",
       "      <td>0.066712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.032198</td>\n",
       "      <td>-0.065506</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.059288</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  category         2         3         4         5         6         7  \\\n",
       "0      APR -0.019496  0.028128 -0.011202  0.040384 -0.076767  0.066849   \n",
       "1      APR -0.002822 -0.045204 -0.016848  0.053938  0.039466  0.065693   \n",
       "2      APR  0.026813 -0.011513 -0.018855  0.043837  0.051915  0.044183   \n",
       "3      APR -0.054095  0.026708 -0.001139  0.000139 -0.019378  0.063965   \n",
       "4      APR  0.044433  0.037699 -0.054426 -0.018390 -0.046270  0.053568   \n",
       "\n",
       "          8         9        10  ...       507       508       509       510  \\\n",
       "0  0.036627 -0.000093 -0.061016  ... -0.046863 -0.003643  0.009728 -0.000819   \n",
       "1 -0.025186 -0.028859  0.004115  ... -0.064839 -0.070530 -0.083139  0.011784   \n",
       "2 -0.004949 -0.061447 -0.014167  ... -0.014507 -0.048853 -0.016512  0.011402   \n",
       "3  0.024880  0.032522 -0.010525  ... -0.019202 -0.047049 -0.063975 -0.059755   \n",
       "4  0.022592 -0.059755  0.066712  ...  0.044229  0.035727  0.030701  0.032198   \n",
       "\n",
       "        511       512       513  language_en  language_es  language_fr  \n",
       "0 -0.073423  0.030665  0.023028            1            0            0  \n",
       "1 -0.018194  0.042321  0.045972            1            0            0  \n",
       "2 -0.006926  0.050542  0.072263            1            0            0  \n",
       "3 -0.069993  0.065400 -0.015806            1            0            0  \n",
       "4 -0.065506  0.039980  0.059288            1            0            0  \n",
       "\n",
       "[5 rows x 516 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the feature vector and the label in two different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_f['category']\n",
    "df_features = df_f.drop(columns=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into training (80%) and test (20%). With the stratify function, it is ensure that there is a good proportion of the 4 labels in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_label, stratify=df_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a classifier an we are going to start with a very basic as Logistic Regression model, because sometimes the simpler the better. There are also other reasons for that, for example, not all the languages have text in all categories, so it is easy to discard a language in some categories, reducing the problem to a kind of \"binary\" classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9929906542056075\n",
      "[[1274    1    3    4]\n",
      " [   0  121    0    3]\n",
      " [  11    1  565    7]\n",
      " [   0    0    3 2715]]\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "              APR       0.99      0.99      0.99      1282\n",
      "Conference_papers       0.98      0.98      0.98       124\n",
      "            PAN11       0.99      0.97      0.98       584\n",
      "        Wikipedia       0.99      1.00      1.00      2718\n",
      "\n",
      "         accuracy                           0.99      4708\n",
      "        macro avg       0.99      0.98      0.99      4708\n",
      "     weighted avg       0.99      0.99      0.99      4708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14505    Wikipedia\n",
      "13514    Wikipedia\n",
      "7682         PAN11\n",
      "11380    Wikipedia\n",
      "21078    Wikipedia\n",
      "1639           APR\n",
      "10915    Wikipedia\n",
      "7446         PAN11\n",
      "23407    Wikipedia\n",
      "4981           APR\n",
      "Name: category, dtype: object\n",
      "['Wikipedia' 'Wikipedia' 'PAN11' 'Wikipedia' 'Wikipedia' 'APR' 'Wikipedia'\n",
      " 'PAN11' 'Wikipedia' 'APR']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0:10])\n",
    "print(y_pred[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Pastel1')\n",
    "plt.title('Confusion matrix', size = 15)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(4)\n",
    "plt.xticks(tick_marks, [\"APR\", \"CP\", \"PAN11\", \"Wikipedia\"], rotation=45, size = 10)\n",
    "plt.yticks(tick_marks, [\"APR\", \"CP\", \"PAN11\", \"Wikipedia\"], size = 10)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Actual label', size = 15)\n",
    "plt.xlabel('Predicted label', size = 15)\n",
    "width, height = cm.shape\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        plt.annotate(str(cm[x][y]), xy=(y, x), \n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resuts are impressively good, even we could consider that the model is overfitted. It classifies correctly almost all the instances, keeping the proportion within categories, even with Conference_papers that are not too many. \n",
    "\n",
    "It is not needed to try to generate a model with a more complex classifier, but it is interesting to try to reduce the accuracy in order to have a more general classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Trying to reduce the overfitting\n",
    "\n",
    "We are going to get the average of each embedding to reduce the amount of specific information from each task, trying to reduce the almost perfect classification from the former training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ea_embeddings_muse_mean.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>embeddings_numpy</th>\n",
       "      <th>embeddings_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i read this book because in my town, everyone ...</td>\n",
       "      <td>APR</td>\n",
       "      <td>en</td>\n",
       "      <td>[-1.94959100e-02  2.81282254e-02 -1.12017877e-...</td>\n",
       "      <td>0.002181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recipes appreciated by the family (small and l...</td>\n",
       "      <td>APR</td>\n",
       "      <td>en</td>\n",
       "      <td>[-0.00282194 -0.04520444 -0.01684837  0.053938...</td>\n",
       "      <td>-0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i say no to ease ..... and not to the author w...</td>\n",
       "      <td>APR</td>\n",
       "      <td>en</td>\n",
       "      <td>[ 2.68126614e-02 -1.15128653e-02 -1.88549645e-...</td>\n",
       "      <td>0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milady has found a good vein: anita blake. bas...</td>\n",
       "      <td>APR</td>\n",
       "      <td>en</td>\n",
       "      <td>[-0.05409476  0.02670753 -0.00113879  0.000138...</td>\n",
       "      <td>-0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460 bc, somewhere in greece: \"gentlemen, i dec...</td>\n",
       "      <td>APR</td>\n",
       "      <td>en</td>\n",
       "      <td>[ 0.04443264  0.03769898 -0.05442553 -0.018389...</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category language  \\\n",
       "0  i read this book because in my town, everyone ...      APR       en   \n",
       "1  recipes appreciated by the family (small and l...      APR       en   \n",
       "2  i say no to ease ..... and not to the author w...      APR       en   \n",
       "3  milady has found a good vein: anita blake. bas...      APR       en   \n",
       "4  460 bc, somewhere in greece: \"gentlemen, i dec...      APR       en   \n",
       "\n",
       "                                    embeddings_numpy  embeddings_mean  \n",
       "0  [-1.94959100e-02  2.81282254e-02 -1.12017877e-...         0.002181  \n",
       "1  [-0.00282194 -0.04520444 -0.01684837  0.053938...        -0.002749  \n",
       "2  [ 2.68126614e-02 -1.15128653e-02 -1.88549645e-...         0.001731  \n",
       "3  [-0.05409476  0.02670753 -0.00113879  0.000138...        -0.000196  \n",
       "4  [ 0.04443264  0.03769898 -0.05442553 -0.018389...         0.000865  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pd.concat to join the new columns with your original dataframe\n",
    "df = pd.concat([df,pd.get_dummies(df['language'], prefix='language')],axis=1)\n",
    "\n",
    "# now drop the original 'country' column (you don't need it anymore)\n",
    "df.drop(['language'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>embeddings_numpy</th>\n",
       "      <th>embeddings_mean</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>language_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i read this book because in my town, everyone ...</td>\n",
       "      <td>APR</td>\n",
       "      <td>[-1.94959100e-02  2.81282254e-02 -1.12017877e-...</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recipes appreciated by the family (small and l...</td>\n",
       "      <td>APR</td>\n",
       "      <td>[-0.00282194 -0.04520444 -0.01684837  0.053938...</td>\n",
       "      <td>-0.002749</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i say no to ease ..... and not to the author w...</td>\n",
       "      <td>APR</td>\n",
       "      <td>[ 2.68126614e-02 -1.15128653e-02 -1.88549645e-...</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milady has found a good vein: anita blake. bas...</td>\n",
       "      <td>APR</td>\n",
       "      <td>[-0.05409476  0.02670753 -0.00113879  0.000138...</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460 bc, somewhere in greece: \"gentlemen, i dec...</td>\n",
       "      <td>APR</td>\n",
       "      <td>[ 0.04443264  0.03769898 -0.05442553 -0.018389...</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category  \\\n",
       "0  i read this book because in my town, everyone ...      APR   \n",
       "1  recipes appreciated by the family (small and l...      APR   \n",
       "2  i say no to ease ..... and not to the author w...      APR   \n",
       "3  milady has found a good vein: anita blake. bas...      APR   \n",
       "4  460 bc, somewhere in greece: \"gentlemen, i dec...      APR   \n",
       "\n",
       "                                    embeddings_numpy  embeddings_mean  \\\n",
       "0  [-1.94959100e-02  2.81282254e-02 -1.12017877e-...         0.002181   \n",
       "1  [-0.00282194 -0.04520444 -0.01684837  0.053938...        -0.002749   \n",
       "2  [ 2.68126614e-02 -1.15128653e-02 -1.88549645e-...         0.001731   \n",
       "3  [-0.05409476  0.02670753 -0.00113879  0.000138...        -0.000196   \n",
       "4  [ 0.04443264  0.03769898 -0.05442553 -0.018389...         0.000865   \n",
       "\n",
       "   language_en  language_es  language_fr  \n",
       "0            1            0            0  \n",
       "1            1            0            0  \n",
       "2            1            0            0  \n",
       "3            1            0            0  \n",
       "4            1            0            0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['text','embeddings_numpy'])\n",
    "df_label = df['category']\n",
    "df_features = df.drop(columns=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings_mean</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>language_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002181</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002749</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001731</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000196</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000865</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   embeddings_mean  language_en  language_es  language_fr\n",
       "0         0.002181            1            0            0\n",
       "1        -0.002749            1            0            0\n",
       "2         0.001731            1            0            0\n",
       "3        -0.000196            1            0            0\n",
       "4         0.000865            1            0            0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    APR\n",
       "1    APR\n",
       "2    APR\n",
       "3    APR\n",
       "4    APR\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_label, stratify=df_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5800764655904843\n",
      "[[  60    0    0 1222]\n",
      " [  38    0    0   86]\n",
      " [  18    0    0  566]\n",
      " [  47    0    0 2671]]\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "              APR       0.37      0.05      0.08      1282\n",
      "Conference_papers       0.00      0.00      0.00       124\n",
      "            PAN11       0.00      0.00      0.00       584\n",
      "        Wikipedia       0.59      0.98      0.74      2718\n",
      "\n",
      "         accuracy                           0.58      4708\n",
      "        macro avg       0.24      0.26      0.20      4708\n",
      "     weighted avg       0.44      0.58      0.45      4708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2_gpu\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Anaconda\\envs\\tf2_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results using the mean of the embedding are way worse. The classifier tends to identify only the most predominant category. The accuracy metrics and confusion matrix are not good, hence this model is discard and we keep using the former one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [1] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation ofword representations in vector space,”Proceedings of Workshop at ICLR,vol. 2013, January 2013\n",
    "* [2] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, andL. Zettlemoyer, “Deep contextualized word representations,” 2018.\n",
    "* [3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-trainingof deep bidirectional transformers for language understanding,” 2018.\n",
    "* [4] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego , Steve Yuan, Chris Tar, Yun-hsuan Sung, Ray Kurzweil. Multilingual Universal Sentence Encoder for Semantic Retrieval. July 2019\n",
    "* [5] Muthuraman Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model. To appear, Repl4NLP@ACL, July 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
